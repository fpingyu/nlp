{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:29:37.046178Z",
     "start_time": "2025-04-10T10:29:36.401648Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nNo module named 'torch.distributed.tensor'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1966\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1968\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\importlib\\__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1030\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:986\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:680\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:850\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:228\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\pipelines\\__init__.py:49\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     36\u001B[0m     CONFIG_NAME,\n\u001B[0;32m     37\u001B[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     47\u001B[0m     logging,\n\u001B[0;32m     48\u001B[0m )\n\u001B[1;32m---> 49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maudio_classification\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AudioClassificationPipeline\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautomatic_speech_recognition\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutomaticSpeechRecognitionPipeline\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\pipelines\\audio_classification.py:21\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m add_end_docstrings, is_torch_available, is_torchaudio_available, logging\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pipeline, build_pipeline_init_args\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\pipelines\\base.py:69\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader, Dataset\n\u001B[1;32m---> 69\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_auto\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoModel\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:41\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensor\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m split_torch_state_dict_into_shards\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch.distributed.tensor'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchtext\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mevaluate\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mticker\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mticker\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\evaluate\\__init__.py:29\u001B[0m\n\u001B[0;32m     25\u001B[0m SCRIPTS_VERSION \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(__version__)\u001B[38;5;241m.\u001B[39mis_devrelease \u001B[38;5;28;01melse\u001B[39;00m __version__\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m version\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation_suite\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m EvaluationSuite\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     31\u001B[0m     AutomaticSpeechRecognitionEvaluator,\n\u001B[0;32m     32\u001B[0m     Evaluator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     41\u001B[0m     evaluator,\n\u001B[0;32m     42\u001B[0m )\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhub\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m push_to_hub\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset, DownloadMode, load_dataset\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Version\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m evaluator\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloading\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m evaluation_module_factory\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfile_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DownloadConfig\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\evaluate\\evaluator\\__init__.py:27\u001B[0m\n\u001B[0;32m     23\u001B[0m     TRANSFORMERS_AVAILABLE \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dict, List\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautomatic_speech_recognition\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutomaticSpeechRecognitionEvaluator\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Evaluator\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_classification\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ImageClassificationEvaluator\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\evaluate\\evaluator\\automatic_speech_recognition.py:22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodule\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m EvaluationModule\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfile_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m add_end_docstrings, add_start_docstrings\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n\u001B[0;32m     25\u001B[0m TASK_DOCUMENTATION \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124m    Examples:\u001B[39m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124m    ```python\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;124m    ```\u001B[39m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mAutomaticSpeechRecognitionEvaluator\u001B[39;00m(Evaluator):\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\evaluate\\evaluator\\base.py:34\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pipeline, pipeline\n\u001B[0;32m     36\u001B[0m     TRANSFORMERS_AVAILABLE \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1055\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1953\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[0;32m   1954\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1955\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1956\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1957\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\import_utils.py:1969\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1968\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1969\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1970\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1971\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1972\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nNo module named 'torch.distributed.tensor'"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:58:42.076504Z",
     "start_time": "2025-04-10T09:58:42.031140Z"
    }
   },
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:58:50.288147Z",
     "start_time": "2025-04-10T09:58:43.647528Z"
    }
   },
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:58:51.533164Z",
     "start_time": "2025-04-10T09:58:51.519150Z"
    }
   },
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:58:54.307899Z",
     "start_time": "2025-04-10T09:58:53.023350Z"
    }
   },
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:58:55.640839Z",
     "start_time": "2025-04-10T09:58:55.633839Z"
    }
   },
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:09.806896Z",
     "start_time": "2025-04-10T09:59:08.404216Z"
    }
   },
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 4880.03 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 6557.44 examples/s]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:14.403202Z",
     "start_time": "2025-04-10T09:59:13.315019Z"
    }
   },
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:15.850478Z",
     "start_time": "2025-04-10T09:59:15.839281Z"
    }
   },
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:17.101675Z",
     "start_time": "2025-04-10T09:59:17.094674Z"
    }
   },
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:18.078604Z",
     "start_time": "2025-04-10T09:59:18.073603Z"
    }
   },
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:19.357950Z",
     "start_time": "2025-04-10T09:59:19.094507Z"
    }
   },
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 12763.53 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 12345.77 examples/s]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:20.967285Z",
     "start_time": "2025-04-10T09:59:20.958268Z"
    }
   },
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:22.723133Z",
     "start_time": "2025-04-10T09:59:22.711118Z"
    }
   },
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:24.189291Z",
     "start_time": "2025-04-10T09:59:24.183292Z"
    }
   },
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:25.674513Z",
     "start_time": "2025-04-10T09:59:25.660514Z"
    }
   },
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:59:27.244147Z",
     "start_time": "2025-04-10T09:59:27.223149Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards\n",
    "        # encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        )\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        return outputs, hidden"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:10.520446Z",
     "start_time": "2025-04-10T07:45:10.509456Z"
    }
   },
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim\n",
    "        )\n",
    "        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_length = encoder_outputs.shape[0]\n",
    "        # repeat decoder hidden state src_length times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # hidden = [batch size, src length, decoder hidden dim]\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch size, src length, decoder hidden dim]\n",
    "        attention = self.v_fc(energy).squeeze(2)\n",
    "        # attention = [batch size, src length]\n",
    "        return torch.softmax(attention, dim=1)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:12.895076Z",
     "start_time": "2025-04-10T07:45:12.882063Z"
    }
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        embedding_dim,\n",
    "        encoder_hidden_dim,\n",
    "        decoder_hidden_dim,\n",
    "        dropout,\n",
    "        attention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)\n",
    "        self.fc_out = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim + embedding_dim, output_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch size, src length]\n",
    "        a = a.unsqueeze(1)\n",
    "        # a = [batch size, 1, src length]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        # weighted = [batch size, 1, encoder hidden dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, encoder hidden dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, (encoder hidden dim * 2) + embedding dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output = [seq length, batch size, decoder hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, decoder hid dim]\n",
    "        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, decoder hidden dim]\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        # this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:15.831152Z",
     "start_time": "2025-04-10T07:45:15.819148Z"
    }
   },
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, decoder hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:22.099775Z",
     "start_time": "2025-04-10T07:45:17.778249Z"
    }
   },
   "source": [
    "input_dim = len(de_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "encoder_hidden_dim = 512\n",
    "decoder_hidden_dim = 512\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    decoder_dropout,\n",
    "    attention,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:24.956794Z",
     "start_time": "2025-04-10T07:45:24.939797Z"
    }
   },
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn_fc): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v_fc): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:27.092568Z",
     "start_time": "2025-04-10T07:45:27.080569Z"
    }
   },
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,518,405 trainable parameters\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:28.597518Z",
     "start_time": "2025-04-10T07:45:28.584519Z"
    }
   },
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:29.961479Z",
     "start_time": "2025-04-10T07:45:29.952479Z"
    }
   },
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:31.612153Z",
     "start_time": "2025-04-10T07:45:31.597150Z"
    }
   },
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"de_ids\"].to(device)\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:45:34.178965Z",
     "start_time": "2025-04-10T07:45:34.164434Z"
    }
   },
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"de_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:58:04.873217Z",
     "start_time": "2025-04-10T07:45:36.064662Z"
    }
   },
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut3-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:17<11:38, 77.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.021 | Train PPL: 151.627\n",
      "\tValid Loss:   4.837 | Valid PPL: 126.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:33<10:10, 76.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.145 | Train PPL:  63.099\n",
      "\tValid Loss:   4.337 | Valid PPL:  76.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:47<08:48, 75.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.441 | Train PPL:  31.217\n",
      "\tValid Loss:   3.712 | Valid PPL:  40.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:01<07:30, 75.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.900 | Train PPL:  18.180\n",
      "\tValid Loss:   3.402 | Valid PPL:  30.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [06:15<06:12, 74.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.512 | Train PPL:  12.329\n",
      "\tValid Loss:   3.318 | Valid PPL:  27.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [07:29<04:57, 74.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.211 | Train PPL:   9.121\n",
      "\tValid Loss:   3.296 | Valid PPL:  27.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [08:43<03:42, 74.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.988 | Train PPL:   7.298\n",
      "\tValid Loss:   3.316 | Valid PPL:  27.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [09:58<02:28, 74.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.760 | Train PPL:   5.813\n",
      "\tValid Loss:   3.395 | Valid PPL:  29.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [11:13<01:14, 74.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.616 | Train PPL:   5.033\n",
      "\tValid Loss:   3.332 | Valid PPL:  27.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:28<00:00, 74.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.468 | Train PPL:   4.342\n",
      "\tValid Loss:   3.434 | Valid PPL:  31.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model.load_state_dict(torch.load(\"tut3-model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:59:13.579333Z",
     "start_time": "2025-04-10T07:59:13.565320Z"
    }
   },
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            de_tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            de_tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            de_tokens = [token.lower() for token in de_tokens]\n",
    "        de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "        ids = de_vocab.lookup_indices(de_tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        encoder_outputs, hidden = model.encoder(tensor)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        attentions = torch.zeros(max_output_length, 1, len(ids))\n",
    "        for i in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, attention = model.decoder(\n",
    "                inputs_tensor, hidden, encoder_outputs\n",
    "            )\n",
    "            attentions[i] = attention\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        en_tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return en_tokens, de_tokens, attentions[: len(en_tokens) - 1]"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:59:15.762076Z",
     "start_time": "2025-04-10T07:59:15.753077Z"
    }
   },
   "source": [
    "def plot_attention(sentence, translation, attention):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    attention = attention.squeeze(1).numpy()\n",
    "    cax = ax.matshow(attention, cmap=\"bone\")\n",
    "    ax.set_xticks(ticks=np.arange(len(sentence)), labels=sentence, rotation=90, size=15)\n",
    "    translation = translation[1:]\n",
    "    ax.set_yticks(ticks=np.arange(len(translation)), labels=translation, size=15)\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:59:17.592199Z",
     "start_time": "2025-04-10T07:59:17.574198Z"
    }
   },
   "source": [
    "sentence = test_data[0][\"de\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.',\n",
       " 'A man in an orange hat starring at something.')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:59:19.542600Z",
     "start_time": "2025-04-10T07:59:19.417361Z"
    }
   },
   "source": [
    "translation, sentence_tokens, attention = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:59:22.523818Z",
     "start_time": "2025-04-10T07:59:22.506821Z"
    }
   },
   "source": [
    "translation"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'a',\n",
       " 'man',\n",
       " 'with',\n",
       " 'an',\n",
       " 'orange',\n",
       " 'hat',\n",
       " 'welding',\n",
       " 'something',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:59:26.346029Z",
     "start_time": "2025-04-10T07:59:26.327018Z"
    }
   },
   "source": [
    "sentence_tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'ein',\n",
       " 'mann',\n",
       " 'mit',\n",
       " 'einem',\n",
       " 'orangefarbenen',\n",
       " 'hut',\n",
       " ',',\n",
       " 'der',\n",
       " 'etwas',\n",
       " 'anstarrt',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-10T07:59:29.470744Z"
    }
   },
   "source": [
    "plot_attention(sentence_tokens, translation, attention)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ein Mann sieht sich einen Film an.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, sentence_tokens, attention = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T09:23:19.833523Z",
     "start_time": "2025-04-10T09:23:19.806223Z"
    }
   },
   "source": [
    "translation"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtranslation\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'translation' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention(sentence_tokens, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:08<00:00, 115.43it/s]\n"
     ]
    }
   ],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"de\"],\n",
    "        model,\n",
    "        en_nlp,\n",
    "        de_nlp,\n",
    "        en_vocab,\n",
    "        de_vocab,\n",
    "        lower,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )[0]\n",
    "    for example in tqdm.tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.2830420566177357,\n",
       " 'precisions': [0.6118114963229776,\n",
       "  0.3586727243225702,\n",
       "  0.21587497792689386,\n",
       "  0.13548324617470464],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0205238168172768,\n",
       " 'translation_length': 13326,\n",
       " 'reference_length': 13058}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
